{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61d16570",
   "metadata": {},
   "source": [
    "# è¤‡æ•°ã‚«ãƒ¼ãƒãƒ«ã®åŠ¹æœæ¤œè¨¼ã‚’UNetã§è¡Œã†\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5af8b71-fb00-4077-a487-01e8b3fa1848",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import segmentation_models_pytorch as smp\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9536a509-2a7e-46d9-9c15-6035e4ecfdd6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10831678",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\PC_User\\Python\\Semantic_Segmentation\\data\\archive\\labels_class_dict.csv\"\n",
    "class_dict = pd.read_csv(path)\n",
    "class_dict.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6722c4f",
   "metadata": {},
   "source": [
    "ä¸Šã‹ã‚‰é †ã« class0 ~ 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae175d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rgb_to_class_mapping():\n",
    "    rgb_to_class = {}\n",
    "    for i, row in enumerate(class_dict.values):\n",
    "        rgb_to_class[(row[1], row[2], row[3])] = i  # RGB -> class_id\n",
    "    return rgb_to_class\n",
    "\n",
    "class StanfordDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir):\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(image_dir, \"*.jpg\")))  \n",
    "        self.label_paths = sorted(glob.glob(os.path.join(label_dir, \"*.png\")))  \n",
    "\n",
    "        # RGB -> ã‚¯ãƒ©ã‚¹IDã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ\n",
    "        self.rgb_to_class = create_rgb_to_class_mapping()\n",
    "\n",
    "        assert len(self.image_paths) == len(self.label_paths), \"ç”»åƒã¨ãƒ©ãƒ™ãƒ«ã®æ•°ãŒä¸€è‡´ã—ã¾ã›ã‚“\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        image = np.array(image)  \n",
    "        \n",
    "        mask = Image.open(self.label_paths[idx]).convert('RGB')\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        # RGBå€¤ã‚’ã‚¯ãƒ©ã‚¹IDã«å¤‰æ›ï¼ˆãªã‚“ã‹ä¸‹æ‰‹ã€‚ã‚‚ã£ã¨ã„ã„æ–¹æ³•ã‚ã‚‹ã¯ãšï¼‰\n",
    "        class_mask = np.zeros(mask.shape[:2], dtype=np.uint8)  # ã‚¯ãƒ©ã‚¹IDç”¨ã®ç©ºã®é…åˆ—\n",
    "        for i in range(mask.shape[0]):  # é«˜ã•\n",
    "            for j in range(mask.shape[1]):  # å¹…\n",
    "                rgb = tuple(mask[i, j])  # (r, g, b) ã®ã‚¿ãƒ—ãƒ«\n",
    "                if rgb in self.rgb_to_class:\n",
    "                    class_mask[i, j] = self.rgb_to_class[rgb]\n",
    "                else:\n",
    "                    class_mask[i, j] = 8  # æœªçŸ¥ã®ã‚¯ãƒ©ã‚¹ã¯8ï¼ˆèƒŒæ™¯ç­‰ï¼‰\n",
    "\n",
    "        return image, class_mask\n",
    "\n",
    "class DatasetWrapper(Dataset):\n",
    "    def __init__(self, subset, transform):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, mask = self.subset[idx]\n",
    "        augmented = self.transform(image=image, mask=mask)\n",
    "        image = augmented['image']\n",
    "        mask = augmented['mask'].long()\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "# --- datasets ---\n",
    "image_path = r\"C:\\Users\\PC_User\\Python\\Semantic_Segmentation\\data\\archive\\images\"\n",
    "label_path = r\"C:\\Users\\PC_User\\Python\\Semantic_Segmentation\\data\\archive\\labels_colored\"\n",
    "\n",
    "full_dataset = StanfordDataset(image_path, label_path)\n",
    "\n",
    "# --- 70% train, 20% val, 10% test ---\n",
    "total_len = len(full_dataset)\n",
    "train_len = int(total_len * 0.7)\n",
    "val_len = int(total_len * 0.2)\n",
    "test_len = total_len - train_len - val_len\n",
    "\n",
    "train_subset, val_subset, test_subset = random_split(\n",
    "    full_dataset, [train_len, val_len, test_len],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "common_transform = [\n",
    "    A.Resize(height=256, width=256),  # Fixed input size\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "    A.RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # ImageNet\n",
    "    ToTensorV2()\n",
    "]\n",
    "\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    *common_transform\n",
    "])\n",
    "\n",
    "valid_transform = A.Compose([\n",
    "    *common_transform\n",
    "])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    *common_transform\n",
    "])\n",
    "\n",
    "# --- Augmentation ---\n",
    "train_datasets = DatasetWrapper(train_subset, transform=train_transform)\n",
    "valid_datasets = DatasetWrapper(val_subset, transform=valid_transform)\n",
    "test_datasets  = DatasetWrapper(test_subset,  transform=test_transform)\n",
    "\n",
    "# --- DataLoader ---\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_datasets, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(valid_datasets, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_datasets, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b832e47-3615-4793-b081-8cb41dfc7e30",
   "metadata": {},
   "source": [
    "## About This Datasets\n",
    "https://www.kaggle.com/datasets/balraj98/stanford-background-dataset <br>\n",
    "ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€LabelMeã€MSRCã€PASCAL VOCã€Geometric Context ãªã©ã®æ—¢å­˜ã®å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰é¸ã°ã‚ŒãŸ 715 æšã®ç”»åƒã§æ§‹æˆã•ã‚Œã¦ã„ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea6303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(img_tensor, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "    mean = torch.tensor(mean).view(-1, 1, 1).to(img_tensor.device)\n",
    "    std = torch.tensor(std).view(-1, 1, 1).to(img_tensor.device)\n",
    "    return img_tensor * std + mean\n",
    "\n",
    "def show_image(image, ax):\n",
    "    image = denormalize(image)\n",
    "    image = torch.clamp(image, 0, 1)\n",
    "    ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "\n",
    "\n",
    "def show_mask(mask, ax):\n",
    "    return ax.imshow(mask.cpu(), cmap='tab10', vmin=0, vmax=8)\n",
    "\n",
    "fig, ax = plt.subplots(4, 4, figsize=(20, 20), constrained_layout=True)\n",
    "cbar_img = None\n",
    "\n",
    "for img_batch, mask_batch in train_loader:\n",
    "    for i in range(8):  \n",
    "        img = img_batch[i]\n",
    "        mask = mask_batch[i]\n",
    "\n",
    "        col = i % 4\n",
    "        row_img = 0 if i < 4 else 1\n",
    "        row_mask = 2 if i < 4 else 3\n",
    "\n",
    "        show_image(img, ax[row_img, col])\n",
    "        cbar_img = show_mask(mask, ax[row_mask, col])  # æœ€å¾Œã®1æšã‚’ä½¿ã£ã¦ã‚«ãƒ©ãƒ¼ãƒãƒ¼\n",
    "    break\n",
    "\n",
    "cbar_ax = fig.add_axes([1.02, 0.15, 0.015, 0.7])  # [left, bottom, width, height]\n",
    "fig.colorbar(cbar_img, cax=cbar_ax)\n",
    "\n",
    "plt.show();\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee406c2-66e5-4acc-b4ed-7f8f57e098b3",
   "metadata": {},
   "source": [
    "## ãƒ–ãƒ­ãƒƒã‚¯ã®å®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961fe10e-32ff-4d75-8617-cae885b1e14e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c9f891-613d-453c-a82f-d626bb9bbcca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, padding: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "        \n",
    "class MultiConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.branch0 = BasicConv2d(in_channels, mid_channels, 1, 0)\n",
    "        self.branch1 = BasicConv2d(in_channels, mid_channels, 3, 1)\n",
    "        self.branch2 = BasicConv2d(in_channels, mid_channels, 5, 2)\n",
    "        self.branch3 = BasicConv2d(in_channels, mid_channels, 7, 3)\n",
    "\n",
    "        self.conv = nn.Conv2d(mid_channels * 4, out_channels, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out0 = self.branch0(x)\n",
    "        out1 = self.branch1(x)\n",
    "        out2 = self.branch2(x)\n",
    "        out3 = self.branch3(x)\n",
    "        out = torch.cat([out0, out1, out2, out3], dim=1)\n",
    "        out = self.conv(out)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77a4c9d-670b-47af-9904-0a5470c81051",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ConvBlockTranspose(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels,  kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels,  kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62919f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiConvBlockTranspose(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, mid_channels, kernel_size=2, stride=2)\n",
    "        self.branch0 = BasicConv2d(mid_channels, mid_channels//2, 1, 0)\n",
    "        self.branch1 = BasicConv2d(mid_channels, mid_channels//2, 3, 1)\n",
    "        self.branch2 = BasicConv2d(mid_channels, mid_channels//2, 5, 2)\n",
    "        self.branch3 = BasicConv2d(mid_channels, mid_channels//2, 7, 3)\n",
    "\n",
    "        self.conv = nn.Conv2d(mid_channels * 2, out_channels, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        out0 = self.branch0(x)\n",
    "        out1 = self.branch1(x)\n",
    "        out2 = self.branch2(x)\n",
    "        out3 = self.branch3(x)\n",
    "        out = torch.cat([out0, out1, out2, out3], dim=1)\n",
    "        out = self.conv(out)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c9064a-6f7d-42b3-874c-130255d67bee",
   "metadata": {},
   "source": [
    "## Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83a7b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.enc1 = ConvBlock(in_channels, 64)\n",
    "        self.enc2 = ConvBlock(64, 128)\n",
    "        self.enc3 = ConvBlock(128, 256)\n",
    "        self.enc4 = ConvBlock(256, 512)\n",
    "\n",
    "        self.bottleneck = ConvBlock(512, 1024)\n",
    "\n",
    "        self.dec4 = ConvBlockTranspose(1024 + 512, 512)\n",
    "        self.dec3 = ConvBlockTranspose(512 + 256, 256)\n",
    "        self.dec2 = ConvBlockTranspose(256 + 128, 128)\n",
    "        self.dec1 = ConvBlockTranspose(128 + 64, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)  # logits\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Encoder \n",
    "        out1 = self.enc1(x)                   # (B, 64, H, W)\n",
    "        e1 = self.pool(out1)                  # (B, 64, H/2, W/2)\n",
    "        out2 = self.enc2(e1)                  # (B, 128, H/2, W/2)\n",
    "        e2 = self.pool(out2)                  # (B, 128, H/4, W/4)\n",
    "        out3 = self.enc3(e2)                  # (B, 256, H/4, W/4)\n",
    "        e3 = self.pool(out3)                  # (B, 256, H/8, W/8)\n",
    "        out4 = self.enc4(e3)                  # (B, 512, H/8, W/8)\n",
    "        e4 = self.pool(out4)                  # (B, 512, H/16, W/16)\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(e4)               # (B, 1024, H/16, W/16)\n",
    "\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        d4 = self.dec4(torch.cat([b, e4], dim=1))     # (B, 512, H/8, H/8)\n",
    "        d3 = self.dec3(torch.cat([d4, e3], dim=1))    # (B, 256, H/4, W/4)\n",
    "        d2 = self.dec2(torch.cat([d3, e2], dim=1))    # (B, 128, H/2, W/2)\n",
    "        d1 = self.dec1(torch.cat([d2, e1], dim=1))    # (B, 64, H, W)\n",
    "\n",
    "        # Final segmentation map\n",
    "        out = self.final_conv(d1)  # (B, num_classes, H, W)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27003d2c-d485-4be9-b941-bacb17161f47",
   "metadata": {},
   "source": [
    "## Multi-kernel UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9416601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_kernel_UNet(nn.Module):\n",
    "    def __init__(self, in_channels: int, num_classes: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc1 = MultiConvBlock(in_channels, 32, 64)\n",
    "        self.enc2 = MultiConvBlock(64, 64, 128)\n",
    "        self.enc3 = MultiConvBlock(128, 128, 256)\n",
    "        self.enc4 = MultiConvBlock(256, 256, 512)\n",
    "\n",
    "        self.bottleneck = MultiConvBlock(512, 512, 1024)\n",
    "\n",
    "        self.dec4 = MultiConvBlockTranspose(1024 + 512, 512, 512)\n",
    "        self.dec3 = MultiConvBlockTranspose(512 + 256, 256, 256)\n",
    "        self.dec2 = MultiConvBlockTranspose(256 + 128, 128, 128)\n",
    "        self.dec1 = MultiConvBlockTranspose(128 + 64, 64, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)  # logits\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Encoder \n",
    "        out1 = self.enc1(x)                   # (B, 64, H, W)\n",
    "        e1 = self.pool(out1)                  # (B, 64, H/2, W/2)\n",
    "        out2 = self.enc2(e1)                  # (B, 128, H/2, W/2)\n",
    "        e2 = self.pool(out2)                  # (B, 128, H/4, W/4)\n",
    "        out3 = self.enc3(e2)                  # (B, 256, H/4, W/4)\n",
    "        e3 = self.pool(out3)                  # (B, 256, H/8, W/8)\n",
    "        out4 = self.enc4(e3)                  # (B, 512, H/8, W/8)\n",
    "        e4 = self.pool(out4)                  # (B, 512, H/16, W/16)\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(e4)               # (B, 1024, H/16, W/16)\n",
    "\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        d4 = self.dec4(torch.cat([b, e4], dim=1))     # (B, 512, H/8, H/8)\n",
    "        d3 = self.dec3(torch.cat([d4, e3], dim=1))    # (B, 256, H/4, W/4)\n",
    "        d2 = self.dec2(torch.cat([d3, e2], dim=1))    # (B, 128, H/2, W/2)\n",
    "        d1 = self.dec1(torch.cat([d2, e1], dim=1))    # (B, 64, H, W)\n",
    "\n",
    "        # Final segmentation map\n",
    "        out = self.final_conv(d1)  # (B, num_classes, H, W)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b2df66-2a90-4558-b39a-e9fc96848d9b",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "IoU ã¯å‰²æ„›ï¼ˆæ¬¡ã™ã‚‹ã“ã¨ãŒã‚ã£ãŸã‚‰ã¡ã‚ƒã‚“ã¨å®Ÿè£…ã™ã‚‹ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beed7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, ignore_index=8):\n",
    "        super().__init__()\n",
    "        self.CELoss = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
    "        self.DiceLoss = smp.losses.DiceLoss(mode='multiclass', ignore_index=ignore_index)\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        ce = self.CELoss(pred, target)\n",
    "        dice = self.DiceLoss(pred, target)\n",
    "        return  ce + dice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574daef6-ebca-4509-b5a4-7af299df86ea",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37853667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_type, num_epochs, train_loader, val_loader):\n",
    "    if model_type == 1:\n",
    "        model = UNet(in_channels=3, num_classes=9).to('cuda')\n",
    "    elif model_type == 2:\n",
    "        model = Multi_kernel_UNet(in_channels=3, num_classes=9).to('cuda')\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model_type {model_type}. Expected 1 or 2\")\n",
    "      \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "    criterion = LossFunction().to('cuda')\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.1)\n",
    "\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, total_train_samples = 0, 0\n",
    "\n",
    "        for img, mask in train_loader:\n",
    "            img = img.to('cuda')\n",
    "            mask = mask.to('cuda')\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            mask_pred = model(img)\n",
    "            \n",
    "            loss = criterion(mask_pred, mask)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * img.size(0)\n",
    "            total_train_samples += img.size(0)\n",
    "\n",
    "        train_loss /= total_train_samples\n",
    "\n",
    "        train_history.append(train_loss)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, total_val_samples = 0, 0\n",
    "\n",
    "            for img, mask in val_loader:\n",
    "                img = img.to('cuda')\n",
    "                mask = mask.to('cuda')\n",
    "\n",
    "                mask_pred = model(img)\n",
    "                loss = criterion(mask_pred, mask)\n",
    "\n",
    "                val_loss += loss.item() * img.size(0)\n",
    "                total_val_samples += img.size(0)\n",
    "\n",
    "            val_loss /= total_val_samples\n",
    " \n",
    "            val_history.append(val_loss)\n",
    "\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            print(f\"[{epoch+1:02d}] Train Loss: {train_loss:.5f} | Valid Loss: {val_loss:.5f}\")\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = model.state_dict()\n",
    "\n",
    "    torch.save(best_model_state, f\"UNet_type{model_type}.pth\")\n",
    "    print(f\"Best model saved with validation loss: {best_val_loss:.5f}\")\n",
    "\n",
    "    if model_type == 1:\n",
    "        best_model = UNet(in_channels=3, num_classes=9).to('cuda')\n",
    "    elif model_type == 2:\n",
    "        best_model = Multi_kernel_UNet(in_channels=3, num_classes=9).to('cuda')\n",
    "\n",
    "    best_model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return best_model, train_history, val_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040405fd-ee74-4d99-9b38-ad10f0848cbd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_history(model_type, num_epoch, train_history, val_history):\n",
    "    \n",
    "    if model_type == 1:\n",
    "        model = 'UNet'\n",
    "    elif model_type == 2:\n",
    "        model = 'Multi kernel UNet'\n",
    "    \n",
    "    train_history = np.array(train_history)\n",
    "    val_history = np.array(val_history)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(np.arange(num_epoch)+1, train_history, label='Train Loss')\n",
    "    plt.plot(np.arange(num_epoch)+1, val_history, label='Val Loss')\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel(f'Loss', fontsize=12)\n",
    "    plt.title(f'{model} Training History', fontsize=12)\n",
    "    plt.xlim(1, num_epoch)\n",
    "    plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07910b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc3250e-3bb2-46f4-9ff4-7f650cc2d6ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "UNet, train_history1, val_history1 = train(1, num_epochs, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e342f44-d546-4b13-990c-e623253759c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(1, num_epochs, train_history1, val_history1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6117163c-ba8f-46f3-83e7-916b122a3856",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Multi_kernel_UNet, train_history2, val_history2 = train(2, num_epochs, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1673375-35b8-4945-9c3e-0cd8795775a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(2, num_epochs, train_history2, val_history2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb2bc1-d65e-4b0a-88a6-c3b025f77f43",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e81d6e-a525-4bc4-bdb4-7af6c42cd641",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    model.eval()\n",
    "    \n",
    "    criterion = LossFunction().to('cuda')\n",
    "\n",
    "    test_loss, total_test_samples = 0, 0\n",
    "    \n",
    "    first_batch_img = None\n",
    "    first_batch_true_mask = None\n",
    "    first_batch_pred_mask = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (img, mask) in enumerate(test_loader):\n",
    "            img = img.to('cuda')\n",
    "            mask = mask.to('cuda')\n",
    "\n",
    "            if idx == 0:\n",
    "                first_batch_img = img.cpu().numpy()\n",
    "                first_batch_true_mask = mask.cpu().numpy()\n",
    "            \n",
    "            mask_pred = model(img)\n",
    "            loss = criterion(mask_pred, mask)\n",
    "\n",
    "            test_loss += loss.item() * img.size(0)\n",
    "            total_test_samples += img.size(0)\n",
    "\n",
    "            if idx == 0:\n",
    "                mask_pred = torch.softmax(mask_pred, dim=1)\n",
    "                first_batch_pred_mask = torch.argmax(mask_pred, dim=1).cpu().numpy()\n",
    "\n",
    "        test_loss /= total_test_samples\n",
    "\n",
    "    return test_loss, first_batch_img, first_batch_true_mask, first_batch_pred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131e90f2-912a-477e-8fc1-32908f684c45",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "test_loss1, first_batch_img, first_batch_true_mask, first_batch_pred_mask1 = test(UNet)\n",
    "test_loss2, first_batch_img, first_batch_true_mask, first_batch_pred_mask2 = test(Multi_kernel_UNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21704728-2b12-4c4f-bff1-0e6a40b5d155",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'UNet | Loss: {test_loss1:05f} | Multi kernel UNet | Loss: {test_loss2:05f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb1e171",
   "metadata": {},
   "source": [
    "## ã¾ã¨ã‚\n",
    "ãƒãƒ«ãƒã‚«ãƒ¼ãƒãƒ«ã‚’å®Ÿè£…ã—ã¦ç²¾åº¦ãŒä¸ŠãŒã‚‹ã‹ã©ã†ã‹ã¯ã€ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚ºæ¬¡ç¬¬ã ã¨è€ƒãˆã‚‰ã‚Œã‚‹ã€‚ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã„ã‚‚ã®ï¼ˆ7Ã—7ï¼‰ãŒã‹ãˆã£ã¦ãƒã‚¤ã‚ºã«ãªã‚‹å¯èƒ½æ€§ã‚‚ã‚ã‚‹ã€‚<br>\n",
    "å­¦ç¿’ã®å®‰å®šæ€§ã«é–¢ã—ã¦ã€ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã„ã‚‚ã®ï¼ˆ7Ã—7ï¼‰ãŒãƒã‚¤ã‚ºã«ãªã£ãŸçµæœã€å­¦ç¿’ãŒæ™®é€šã® UNet ã‚ˆã‚Šã‚‚ä¸å®‰å®šã«ãªã£ãŸã®ã§ã¯ãªã„ã ã‚ã†ã‹ã€‚æ€ã£ãŸã‚ˆã‚Šç²¾åº¦ãŒä¸ŠãŒã‚‰ãªã‹ã£ãŸã®ã‚‚ãã®ã›ã„ã‹ã€‚<br>\n",
    "ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã®ç‰¹å¾´ã‚’é›†ç´„ã™ã‚‹ã“ã¨ã§ 1 ã¤ã®å±€æ‰€ãƒ‘ã‚¿ãƒ¼ãƒ³ã«éåº¦ã«ä¾å­˜ã™ã‚‹ãƒªã‚¹ã‚¯ãŒä¸‹ãŒã‚‹ãŸã‚ã€éå­¦ç¿’ã«å¼·ããªã‚Šã‚„ã™ã„ã¨æ€ã£ãŸã®ã ãŒï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’çš„ãªåŠ¹æœï¼‰ã€ãã®ã‚ˆã†ãªåŠ¹æœã¯ä»Šå›ã¯ç¢ºèªã§ããªã‹ã£ãŸã€‚\n",
    "### ç²¾åº¦å‘ä¸Šã®è¦å› \n",
    "- ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«è¨­è¨ˆã ã‘ã§ãªãã€**ã€Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¢—åŠ ã€** ã¯æ€§èƒ½å‘ä¸Šã®å¤§ããªè¦å› ã®ä¸€ã¤ã ã¨è€ƒãˆã‚‰ã‚Œã‚‹ã€‚\n",
    "- ãŸã ã—ã€ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«è¨­è¨ˆã¯ **ã€Œå¤šæ§˜ãªç‰¹å¾´æŠ½å‡ºã€** ã¨ã„ã†ä»˜åŠ ä¾¡å€¤ãŒã‚ã‚‹ã®ã§ã€å˜ãªã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°å¢—åŠ ä»¥ä¸Šã®ãƒ¡ãƒªãƒƒãƒˆã‚‚æœŸå¾…ã§ãã‚‹ã¯ãšã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d78383-0121-470a-a541-6b299dfde5d3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ignore_mask = first_batch_true_mask == 8\n",
    "first_batch_pred_mask1[ignore_mask] = 8\n",
    "first_batch_pred_mask2[ignore_mask] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ac3a6c-a2e4-4d26-be25-7e82b3072103",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def denormalize(img_array, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "    mean = np.array(mean).reshape(-1, 1, 1)\n",
    "    std = np.array(std).reshape(-1, 1, 1)\n",
    "    return img_array * std + mean\n",
    "\n",
    "def show_image(image, ax):\n",
    "    image = denormalize(image)\n",
    "    image = np.clip(image, 0, 1)\n",
    "    ax.imshow(image.transpose(1, 2, 0))\n",
    "\n",
    "def plot_mask(mask):\n",
    "    fig, ax = plt.subplots(2, 4, figsize=(20, 10), constrained_layout=True)\n",
    "    cbar_img = None\n",
    "    ax = ax.flatten()\n",
    "    for i in range(8):\n",
    "        img = mask[i]\n",
    "        cbar_img = ax[i].imshow(img, cmap='tab10', vmin=0, vmax=8)\n",
    "    cbar_ax = fig.add_axes([1.02, 0.15, 0.015, 0.7])\n",
    "    fig.colorbar(cbar_img, cax=cbar_ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbedc4d-3871-4d34-9db1-cc765295a8f6",
   "metadata": {},
   "source": [
    "## First batch image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92baffb7-1b6c-43c2-bb4f-fd02313e084a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 4, figsize=(20, 10))\n",
    "ax = ax.flatten()\n",
    "for i in range(8):\n",
    "    image = first_batch_img[i]\n",
    "    show_image(image, ax[i])\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc85a535-a5d3-489e-bfb5-3a0ed2bb2ca1",
   "metadata": {},
   "source": [
    "## True mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69ac3f8-676f-4326-9c3b-9a0fd0bb79fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mask(first_batch_true_mask);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f337b101-6bec-48a2-8791-21ef64c738cb",
   "metadata": {},
   "source": [
    "## UNet predict mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80a6b5e-ca85-4431-8fde-94b37588c865",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mask(first_batch_pred_mask1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a25f096-843f-4e9d-b157-307548495724",
   "metadata": {},
   "source": [
    "## Multi kernel UNet predict mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534140ca-a0ce-4854-8613-ecc85458481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mask(first_batch_pred_mask2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f40761",
   "metadata": {},
   "source": [
    "## æ„Ÿæƒ³\n",
    "ãƒãƒ«ãƒã‚«ãƒ¼ãƒãƒ«è¨­è¨ˆã®ã»ã†ãŒç²¾åº¦ãŒã„ã„ãŒã€ãã‚Œã§ã‚‚ç²¾åº¦ãŒä½ã„ã€‚ç²¾åº¦ã‚ˆãã‚„ã£ã¦ã„ã‚‹äººãŸã¡ã¯ã©ã‚“ãªå·¥å¤«ã‚’ã—ã¦ã„ã‚‹ã‹ã€æå¤±é–¢æ•°å«ã‚ã¦æ°—ã«ãªã£ãŸã€‚<br>\n",
    "ä»Šåº¦ã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å†…å®¹ã‚’ç¢ºèªã—ã¦ã¿ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70913c4",
   "metadata": {},
   "source": [
    "\n",
    "## ğŸš˜ è‡ªå‹•é‹è»¢ã§ã‚ˆãä½¿ã‚ã‚Œã‚‹ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«\n",
    "\n",
    "### âœ… 1. **DeepLab v3+**\n",
    "\n",
    "* **ç‰¹å¾´**ï¼šAtrous Convolutionï¼ˆç©ºæ´ç•³ã¿è¾¼ã¿ï¼‰ + Encoder-Decoder æ§‹é€ \n",
    "* **åˆ©ç‚¹**ï¼šé«˜ç²¾åº¦ã€‚é æ–¹ã®ç‰©ä½“ã‚„ç´°ã‹ã„æ§‹é€ ã«ã‚‚å¼·ã„\n",
    "* **ç”¨é€”**ï¼šè‡ªå‹•é‹è»¢ã®ç ”ç©¶ãƒ»ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã«ã‚ˆãä½¿ã‚ã‚Œã‚‹\n",
    "* **å®Ÿè£…**ï¼š`torchvision.models.segmentation.deeplabv3_resnet101` ãªã©\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… 2. **PSPNet (Pyramid Scene Parsing Network)**\n",
    "\n",
    "* **ç‰¹å¾´**ï¼šãƒ”ãƒ©ãƒŸãƒƒãƒ‰ãƒ—ãƒ¼ãƒªãƒ³ã‚°ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§åºƒç¯„å›²ãªæ–‡è„ˆæƒ…å ±ã‚’å–å¾—\n",
    "* **åˆ©ç‚¹**ï¼šã‚·ãƒ¼ãƒ³å…¨ä½“ã‚’æŠŠæ¡ã—ãŸã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã«å¼·ã„\n",
    "* **ç”¨é€”**ï¼šéƒ½å¸‚ã‚·ãƒ¼ãƒ³ç†è§£ï¼ˆä¾‹ï¼šCityscapesï¼‰ã«å¼·ã„å®Ÿç¸¾\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… 3. **ENet**\n",
    "\n",
    "* **ç‰¹å¾´**ï¼šè»½é‡ãªãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ï¼ˆSpeedé‡è¦–ï¼‰\n",
    "* **åˆ©ç‚¹**ï¼šçµ„ã¿è¾¼ã¿ãƒ‡ãƒã‚¤ã‚¹ï¼ˆNVIDIA Jetsonç­‰ï¼‰ã«å‘ã\n",
    "* **ç”¨é€”**ï¼šå®Ÿèµ°è¡Œè»Šä¸¡ã§ã®ãƒ†ã‚¹ãƒˆã‚„å°å‹ã‚·ã‚¹ãƒ†ãƒ ã«æœ€é©\n",
    "* **ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆ**ï¼š> 60 FPS on embedded devices\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… 4. **BiSeNet (Bilateral Segmentation Network)**\n",
    "\n",
    "* **ç‰¹å¾´**ï¼šç©ºé–“æƒ…å ±ã¨æ–‡è„ˆæƒ…å ±ã‚’ä¸¦åˆ—ã«æ‰±ã†2ãƒ–ãƒ©ãƒ³ãƒæ§‹æˆ\n",
    "* **åˆ©ç‚¹**ï¼š**ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§ã¨é«˜ç²¾åº¦ã®ä¸¡ç«‹**\n",
    "* **BiSeNetV2** ã¯ã•ã‚‰ã«é«˜åŠ¹ç‡è¨­è¨ˆ\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… 5. **SegFormerï¼ˆNVIDIA, 2021ï¼‰**\n",
    "\n",
    "* **ç‰¹å¾´**ï¼š**Transformerãƒ™ãƒ¼ã‚¹**ã€‚è»½é‡ & é«˜ç²¾åº¦\n",
    "* **åˆ©ç‚¹**ï¼šãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã«CNNã‚’ä½¿ã‚ãšã«Transformerã§é«˜æ€§èƒ½\n",
    "* **ç”¨é€”**ï¼šSOTAæ€§èƒ½ + å°å‹åŒ–ï¼ˆreal-timeå¯¾å¿œï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š ãƒ¢ãƒ‡ãƒ«é¸æŠã®ç›®å®‰\n",
    "\n",
    "| ãƒ¢ãƒ‡ãƒ«         | ç²¾åº¦ | æ¨è«–é€Ÿåº¦ | å®Ÿç”¨æ€§ | å‚™è€ƒ          |\n",
    "| ----------- | -- | ---- | --- | ----------- |\n",
    "| DeepLab v3+ | â—  | â–³    | â—   | ç²¾åº¦é‡è¦–        |\n",
    "| PSPNet      | â—  | â–³    | â—   | éƒ½å¸‚é¢¨æ™¯å‘ã      |\n",
    "| ENet        | â–³  | â—    | â—¯   | çµ„ã¿è¾¼ã¿å‘ã‘      |\n",
    "| BiSeNetV2   | â—  | â—    | â—   | å®Ÿè»Šæ­è¼‰ã®é¸æŠè‚¢ã«ã‚‚  |\n",
    "| SegFormer   | â—  | â—    | â—   | æœ€æ–°ã€‚è»½é‡ & é«˜æ€§èƒ½ |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ™ï¸ ã‚ˆãä½¿ã‚ã‚Œã‚‹è‡ªå‹•é‹è»¢ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "\n",
    "| ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ         | ç‰¹å¾´                |\n",
    "| -------------- | ----------------- |\n",
    "| **Cityscapes** | éƒ½å¸‚éƒ¨ã®é«˜è§£åƒåº¦ç”»åƒï¼ˆ19ã‚¯ãƒ©ã‚¹ï¼‰ |\n",
    "| **CamVid**     | é“è·¯èµ°è¡Œã‚·ãƒ¼ãƒ³ï¼ˆè»Šè¼‰ã‚«ãƒ¡ãƒ©æ˜ åƒï¼‰  |\n",
    "| **KITTI**      | è»Šè¼‰ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ©ãƒ™ãƒ«å°‘ãªã‚ï¼‰ |\n",
    "| **BDD100K**    | å¤šæ§˜ãªæ™‚é–“å¸¯ãƒ»å¤©å€™ã§å¤§è¦æ¨¡     |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ ã¾ã¨ã‚\n",
    "\n",
    "* **ç²¾åº¦é‡è¦–ï¼ˆç ”ç©¶ç”¨é€”ï¼‰**ï¼šDeepLab v3+, PSPNet\n",
    "* **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ï¼ˆå®Ÿé‹ç”¨ãƒ»çµ„ã¿è¾¼ã¿ï¼‰**ï¼šENet, BiSeNetV2\n",
    "* **æ¬¡ä¸–ä»£ãƒ¢ãƒ‡ãƒ«ã«æ³¨ç›®**ï¼šSegFormer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
