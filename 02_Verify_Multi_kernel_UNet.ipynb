{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61d16570",
   "metadata": {},
   "source": [
    "# 複数カーネルの効果検証をUNetで行う\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5af8b71-fb00-4077-a487-01e8b3fa1848",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import segmentation_models_pytorch as smp\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9536a509-2a7e-46d9-9c15-6035e4ecfdd6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10831678",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\PC_User\\Python\\Semantic_Segmentation\\data\\archive\\labels_class_dict.csv\"\n",
    "class_dict = pd.read_csv(path)\n",
    "class_dict.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6722c4f",
   "metadata": {},
   "source": [
    "上から順に class0 ~ 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae175d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rgb_to_class_mapping():\n",
    "    rgb_to_class = {}\n",
    "    for i, row in enumerate(class_dict.values):\n",
    "        rgb_to_class[(row[1], row[2], row[3])] = i  # RGB -> class_id\n",
    "    return rgb_to_class\n",
    "\n",
    "class StanfordDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir):\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(image_dir, \"*.jpg\")))  \n",
    "        self.label_paths = sorted(glob.glob(os.path.join(label_dir, \"*.png\")))  \n",
    "\n",
    "        # RGB -> クラスIDのマッピングを作成\n",
    "        self.rgb_to_class = create_rgb_to_class_mapping()\n",
    "\n",
    "        assert len(self.image_paths) == len(self.label_paths), \"画像とラベルの数が一致しません\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        image = np.array(image)  \n",
    "        \n",
    "        mask = Image.open(self.label_paths[idx]).convert('RGB')\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        # RGB値をクラスIDに変換（なんか下手。もっといい方法あるはず）\n",
    "        class_mask = np.zeros(mask.shape[:2], dtype=np.uint8)  # クラスID用の空の配列\n",
    "        for i in range(mask.shape[0]):  # 高さ\n",
    "            for j in range(mask.shape[1]):  # 幅\n",
    "                rgb = tuple(mask[i, j])  # (r, g, b) のタプル\n",
    "                if rgb in self.rgb_to_class:\n",
    "                    class_mask[i, j] = self.rgb_to_class[rgb]\n",
    "                else:\n",
    "                    class_mask[i, j] = 8  # 未知のクラスは8（背景等）\n",
    "\n",
    "        return image, class_mask\n",
    "\n",
    "class DatasetWrapper(Dataset):\n",
    "    def __init__(self, subset, transform):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, mask = self.subset[idx]\n",
    "        augmented = self.transform(image=image, mask=mask)\n",
    "        image = augmented['image']\n",
    "        mask = augmented['mask'].long()\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "# --- datasets ---\n",
    "image_path = r\"C:\\Users\\PC_User\\Python\\Semantic_Segmentation\\data\\archive\\images\"\n",
    "label_path = r\"C:\\Users\\PC_User\\Python\\Semantic_Segmentation\\data\\archive\\labels_colored\"\n",
    "\n",
    "full_dataset = StanfordDataset(image_path, label_path)\n",
    "\n",
    "# --- 70% train, 20% val, 10% test ---\n",
    "total_len = len(full_dataset)\n",
    "train_len = int(total_len * 0.7)\n",
    "val_len = int(total_len * 0.2)\n",
    "test_len = total_len - train_len - val_len\n",
    "\n",
    "train_subset, val_subset, test_subset = random_split(\n",
    "    full_dataset, [train_len, val_len, test_len],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "common_transform = [\n",
    "    A.Resize(height=256, width=256),  # Fixed input size\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "    A.RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # ImageNet\n",
    "    ToTensorV2()\n",
    "]\n",
    "\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    *common_transform\n",
    "])\n",
    "\n",
    "valid_transform = A.Compose([\n",
    "    *common_transform\n",
    "])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    *common_transform\n",
    "])\n",
    "\n",
    "# --- Augmentation ---\n",
    "train_datasets = DatasetWrapper(train_subset, transform=train_transform)\n",
    "valid_datasets = DatasetWrapper(val_subset, transform=valid_transform)\n",
    "test_datasets  = DatasetWrapper(test_subset,  transform=test_transform)\n",
    "\n",
    "# --- DataLoader ---\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_datasets, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(valid_datasets, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_datasets, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b832e47-3615-4793-b081-8cb41dfc7e30",
   "metadata": {},
   "source": [
    "## About This Datasets\n",
    "https://www.kaggle.com/datasets/balraj98/stanford-background-dataset <br>\n",
    "このデータセットは、LabelMe、MSRC、PASCAL VOC、Geometric Context などの既存の公開データセットから選ばれた 715 枚の画像で構成されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea6303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(img_tensor, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "    mean = torch.tensor(mean).view(-1, 1, 1).to(img_tensor.device)\n",
    "    std = torch.tensor(std).view(-1, 1, 1).to(img_tensor.device)\n",
    "    return img_tensor * std + mean\n",
    "\n",
    "def show_image(image, ax):\n",
    "    image = denormalize(image)\n",
    "    image = torch.clamp(image, 0, 1)\n",
    "    ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "\n",
    "\n",
    "def show_mask(mask, ax):\n",
    "    return ax.imshow(mask.cpu(), cmap='tab10', vmin=0, vmax=8)\n",
    "\n",
    "fig, ax = plt.subplots(4, 4, figsize=(20, 20), constrained_layout=True)\n",
    "cbar_img = None\n",
    "\n",
    "for img_batch, mask_batch in train_loader:\n",
    "    for i in range(8):  \n",
    "        img = img_batch[i]\n",
    "        mask = mask_batch[i]\n",
    "\n",
    "        col = i % 4\n",
    "        row_img = 0 if i < 4 else 1\n",
    "        row_mask = 2 if i < 4 else 3\n",
    "\n",
    "        show_image(img, ax[row_img, col])\n",
    "        cbar_img = show_mask(mask, ax[row_mask, col])  # 最後の1枚を使ってカラーバー\n",
    "    break\n",
    "\n",
    "cbar_ax = fig.add_axes([1.02, 0.15, 0.015, 0.7])  # [left, bottom, width, height]\n",
    "fig.colorbar(cbar_img, cax=cbar_ax)\n",
    "\n",
    "plt.show();\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee406c2-66e5-4acc-b4ed-7f8f57e098b3",
   "metadata": {},
   "source": [
    "## ブロックの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961fe10e-32ff-4d75-8617-cae885b1e14e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c9f891-613d-453c-a82f-d626bb9bbcca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, padding: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "        \n",
    "class MultiConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.branch0 = BasicConv2d(in_channels, mid_channels, 1, 0)\n",
    "        self.branch1 = BasicConv2d(in_channels, mid_channels, 3, 1)\n",
    "        self.branch2 = BasicConv2d(in_channels, mid_channels, 5, 2)\n",
    "        self.branch3 = BasicConv2d(in_channels, mid_channels, 7, 3)\n",
    "\n",
    "        self.conv = nn.Conv2d(mid_channels * 4, out_channels, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out0 = self.branch0(x)\n",
    "        out1 = self.branch1(x)\n",
    "        out2 = self.branch2(x)\n",
    "        out3 = self.branch3(x)\n",
    "        out = torch.cat([out0, out1, out2, out3], dim=1)\n",
    "        out = self.conv(out)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77a4c9d-670b-47af-9904-0a5470c81051",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ConvBlockTranspose(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels,  kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels,  kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62919f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiConvBlockTranspose(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, mid_channels, kernel_size=2, stride=2)\n",
    "        self.branch0 = BasicConv2d(mid_channels, mid_channels//2, 1, 0)\n",
    "        self.branch1 = BasicConv2d(mid_channels, mid_channels//2, 3, 1)\n",
    "        self.branch2 = BasicConv2d(mid_channels, mid_channels//2, 5, 2)\n",
    "        self.branch3 = BasicConv2d(mid_channels, mid_channels//2, 7, 3)\n",
    "\n",
    "        self.conv = nn.Conv2d(mid_channels * 2, out_channels, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        out0 = self.branch0(x)\n",
    "        out1 = self.branch1(x)\n",
    "        out2 = self.branch2(x)\n",
    "        out3 = self.branch3(x)\n",
    "        out = torch.cat([out0, out1, out2, out3], dim=1)\n",
    "        out = self.conv(out)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c9064a-6f7d-42b3-874c-130255d67bee",
   "metadata": {},
   "source": [
    "## Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83a7b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.enc1 = ConvBlock(in_channels, 64)\n",
    "        self.enc2 = ConvBlock(64, 128)\n",
    "        self.enc3 = ConvBlock(128, 256)\n",
    "        self.enc4 = ConvBlock(256, 512)\n",
    "\n",
    "        self.bottleneck = ConvBlock(512, 1024)\n",
    "\n",
    "        self.dec4 = ConvBlockTranspose(1024 + 512, 512)\n",
    "        self.dec3 = ConvBlockTranspose(512 + 256, 256)\n",
    "        self.dec2 = ConvBlockTranspose(256 + 128, 128)\n",
    "        self.dec1 = ConvBlockTranspose(128 + 64, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)  # logits\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Encoder \n",
    "        out1 = self.enc1(x)                   # (B, 64, H, W)\n",
    "        e1 = self.pool(out1)                  # (B, 64, H/2, W/2)\n",
    "        out2 = self.enc2(e1)                  # (B, 128, H/2, W/2)\n",
    "        e2 = self.pool(out2)                  # (B, 128, H/4, W/4)\n",
    "        out3 = self.enc3(e2)                  # (B, 256, H/4, W/4)\n",
    "        e3 = self.pool(out3)                  # (B, 256, H/8, W/8)\n",
    "        out4 = self.enc4(e3)                  # (B, 512, H/8, W/8)\n",
    "        e4 = self.pool(out4)                  # (B, 512, H/16, W/16)\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(e4)               # (B, 1024, H/16, W/16)\n",
    "\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        d4 = self.dec4(torch.cat([b, e4], dim=1))     # (B, 512, H/8, H/8)\n",
    "        d3 = self.dec3(torch.cat([d4, e3], dim=1))    # (B, 256, H/4, W/4)\n",
    "        d2 = self.dec2(torch.cat([d3, e2], dim=1))    # (B, 128, H/2, W/2)\n",
    "        d1 = self.dec1(torch.cat([d2, e1], dim=1))    # (B, 64, H, W)\n",
    "\n",
    "        # Final segmentation map\n",
    "        out = self.final_conv(d1)  # (B, num_classes, H, W)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27003d2c-d485-4be9-b941-bacb17161f47",
   "metadata": {},
   "source": [
    "## Multi-kernel UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9416601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_kernel_UNet(nn.Module):\n",
    "    def __init__(self, in_channels: int, num_classes: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc1 = MultiConvBlock(in_channels, 32, 64)\n",
    "        self.enc2 = MultiConvBlock(64, 64, 128)\n",
    "        self.enc3 = MultiConvBlock(128, 128, 256)\n",
    "        self.enc4 = MultiConvBlock(256, 256, 512)\n",
    "\n",
    "        self.bottleneck = MultiConvBlock(512, 512, 1024)\n",
    "\n",
    "        self.dec4 = MultiConvBlockTranspose(1024 + 512, 512, 512)\n",
    "        self.dec3 = MultiConvBlockTranspose(512 + 256, 256, 256)\n",
    "        self.dec2 = MultiConvBlockTranspose(256 + 128, 128, 128)\n",
    "        self.dec1 = MultiConvBlockTranspose(128 + 64, 64, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)  # logits\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Encoder \n",
    "        out1 = self.enc1(x)                   # (B, 64, H, W)\n",
    "        e1 = self.pool(out1)                  # (B, 64, H/2, W/2)\n",
    "        out2 = self.enc2(e1)                  # (B, 128, H/2, W/2)\n",
    "        e2 = self.pool(out2)                  # (B, 128, H/4, W/4)\n",
    "        out3 = self.enc3(e2)                  # (B, 256, H/4, W/4)\n",
    "        e3 = self.pool(out3)                  # (B, 256, H/8, W/8)\n",
    "        out4 = self.enc4(e3)                  # (B, 512, H/8, W/8)\n",
    "        e4 = self.pool(out4)                  # (B, 512, H/16, W/16)\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(e4)               # (B, 1024, H/16, W/16)\n",
    "\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        d4 = self.dec4(torch.cat([b, e4], dim=1))     # (B, 512, H/8, H/8)\n",
    "        d3 = self.dec3(torch.cat([d4, e3], dim=1))    # (B, 256, H/4, W/4)\n",
    "        d2 = self.dec2(torch.cat([d3, e2], dim=1))    # (B, 128, H/2, W/2)\n",
    "        d1 = self.dec1(torch.cat([d2, e1], dim=1))    # (B, 64, H, W)\n",
    "\n",
    "        # Final segmentation map\n",
    "        out = self.final_conv(d1)  # (B, num_classes, H, W)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b2df66-2a90-4558-b39a-e9fc96848d9b",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "IoU は割愛（次することがあったらちゃんと実装する）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beed7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, ignore_index=8):\n",
    "        super().__init__()\n",
    "        self.CELoss = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
    "        self.DiceLoss = smp.losses.DiceLoss(mode='multiclass', ignore_index=ignore_index)\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        ce = self.CELoss(pred, target)\n",
    "        dice = self.DiceLoss(pred, target)\n",
    "        return  ce + dice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574daef6-ebca-4509-b5a4-7af299df86ea",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37853667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_type, num_epochs, train_loader, val_loader):\n",
    "    if model_type == 1:\n",
    "        model = UNet(in_channels=3, num_classes=9).to('cuda')\n",
    "    elif model_type == 2:\n",
    "        model = Multi_kernel_UNet(in_channels=3, num_classes=9).to('cuda')\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model_type {model_type}. Expected 1 or 2\")\n",
    "      \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "    criterion = LossFunction().to('cuda')\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.1)\n",
    "\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, total_train_samples = 0, 0\n",
    "\n",
    "        for img, mask in train_loader:\n",
    "            img = img.to('cuda')\n",
    "            mask = mask.to('cuda')\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            mask_pred = model(img)\n",
    "            \n",
    "            loss = criterion(mask_pred, mask)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * img.size(0)\n",
    "            total_train_samples += img.size(0)\n",
    "\n",
    "        train_loss /= total_train_samples\n",
    "\n",
    "        train_history.append(train_loss)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, total_val_samples = 0, 0\n",
    "\n",
    "            for img, mask in val_loader:\n",
    "                img = img.to('cuda')\n",
    "                mask = mask.to('cuda')\n",
    "\n",
    "                mask_pred = model(img)\n",
    "                loss = criterion(mask_pred, mask)\n",
    "\n",
    "                val_loss += loss.item() * img.size(0)\n",
    "                total_val_samples += img.size(0)\n",
    "\n",
    "            val_loss /= total_val_samples\n",
    " \n",
    "            val_history.append(val_loss)\n",
    "\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            print(f\"[{epoch+1:02d}] Train Loss: {train_loss:.5f} | Valid Loss: {val_loss:.5f}\")\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = model.state_dict()\n",
    "\n",
    "    torch.save(best_model_state, f\"UNet_type{model_type}.pth\")\n",
    "    print(f\"Best model saved with validation loss: {best_val_loss:.5f}\")\n",
    "\n",
    "    if model_type == 1:\n",
    "        best_model = UNet(in_channels=3, num_classes=9).to('cuda')\n",
    "    elif model_type == 2:\n",
    "        best_model = Multi_kernel_UNet(in_channels=3, num_classes=9).to('cuda')\n",
    "\n",
    "    best_model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return best_model, train_history, val_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040405fd-ee74-4d99-9b38-ad10f0848cbd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_history(model_type, num_epoch, train_history, val_history):\n",
    "    \n",
    "    if model_type == 1:\n",
    "        model = 'UNet'\n",
    "    elif model_type == 2:\n",
    "        model = 'Multi kernel UNet'\n",
    "    \n",
    "    train_history = np.array(train_history)\n",
    "    val_history = np.array(val_history)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(np.arange(num_epoch)+1, train_history, label='Train Loss')\n",
    "    plt.plot(np.arange(num_epoch)+1, val_history, label='Val Loss')\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel(f'Loss', fontsize=12)\n",
    "    plt.title(f'{model} Training History', fontsize=12)\n",
    "    plt.xlim(1, num_epoch)\n",
    "    plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07910b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc3250e-3bb2-46f4-9ff4-7f650cc2d6ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "UNet, train_history1, val_history1 = train(1, num_epochs, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e342f44-d546-4b13-990c-e623253759c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(1, num_epochs, train_history1, val_history1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6117163c-ba8f-46f3-83e7-916b122a3856",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Multi_kernel_UNet, train_history2, val_history2 = train(2, num_epochs, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1673375-35b8-4945-9c3e-0cd8795775a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(2, num_epochs, train_history2, val_history2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb2bc1-d65e-4b0a-88a6-c3b025f77f43",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e81d6e-a525-4bc4-bdb4-7af6c42cd641",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    model.eval()\n",
    "    \n",
    "    criterion = LossFunction().to('cuda')\n",
    "\n",
    "    test_loss, total_test_samples = 0, 0\n",
    "    \n",
    "    first_batch_img = None\n",
    "    first_batch_true_mask = None\n",
    "    first_batch_pred_mask = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (img, mask) in enumerate(test_loader):\n",
    "            img = img.to('cuda')\n",
    "            mask = mask.to('cuda')\n",
    "\n",
    "            if idx == 0:\n",
    "                first_batch_img = img.cpu().numpy()\n",
    "                first_batch_true_mask = mask.cpu().numpy()\n",
    "            \n",
    "            mask_pred = model(img)\n",
    "            loss = criterion(mask_pred, mask)\n",
    "\n",
    "            test_loss += loss.item() * img.size(0)\n",
    "            total_test_samples += img.size(0)\n",
    "\n",
    "            if idx == 0:\n",
    "                mask_pred = torch.softmax(mask_pred, dim=1)\n",
    "                first_batch_pred_mask = torch.argmax(mask_pred, dim=1).cpu().numpy()\n",
    "\n",
    "        test_loss /= total_test_samples\n",
    "\n",
    "    return test_loss, first_batch_img, first_batch_true_mask, first_batch_pred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131e90f2-912a-477e-8fc1-32908f684c45",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "test_loss1, first_batch_img, first_batch_true_mask, first_batch_pred_mask1 = test(UNet)\n",
    "test_loss2, first_batch_img, first_batch_true_mask, first_batch_pred_mask2 = test(Multi_kernel_UNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21704728-2b12-4c4f-bff1-0e6a40b5d155",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'UNet | Loss: {test_loss1:05f} | Multi kernel UNet | Loss: {test_loss2:05f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb1e171",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "マルチカーネルを実装して精度が上がるかどうかは、カーネルサイズ次第だと考えられる。カーネルサイズが大きいもの（7×7）がかえってノイズになる可能性もある。<br>\n",
    "学習の安定性に関して、カーネルサイズが大きいもの（7×7）がノイズになった結果、学習が普通の UNet よりも不安定になったのではないだろうか。思ったより精度が上がらなかったのもそのせいか。<br>\n",
    "マルチスケールの特徴を集約することで 1 つの局所パターンに過度に依存するリスクが下がるため、過学習に強くなりやすいと思ったのだが（アンサンブル学習的な効果）、そのような効果は今回は確認できなかった。\n",
    "### 精度向上の要因\n",
    "- マルチスケール設計だけでなく、**「パラメータの増加」** は性能向上の大きな要因の一つだと考えられる。\n",
    "- ただし、マルチスケール設計は **「多様な特徴抽出」** という付加価値があるので、単なるパラメータ数増加以上のメリットも期待できるはず。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d78383-0121-470a-a541-6b299dfde5d3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ignore_mask = first_batch_true_mask == 8\n",
    "first_batch_pred_mask1[ignore_mask] = 8\n",
    "first_batch_pred_mask2[ignore_mask] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ac3a6c-a2e4-4d26-be25-7e82b3072103",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def denormalize(img_array, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "    mean = np.array(mean).reshape(-1, 1, 1)\n",
    "    std = np.array(std).reshape(-1, 1, 1)\n",
    "    return img_array * std + mean\n",
    "\n",
    "def show_image(image, ax):\n",
    "    image = denormalize(image)\n",
    "    image = np.clip(image, 0, 1)\n",
    "    ax.imshow(image.transpose(1, 2, 0))\n",
    "\n",
    "def plot_mask(mask):\n",
    "    fig, ax = plt.subplots(2, 4, figsize=(20, 10), constrained_layout=True)\n",
    "    cbar_img = None\n",
    "    ax = ax.flatten()\n",
    "    for i in range(8):\n",
    "        img = mask[i]\n",
    "        cbar_img = ax[i].imshow(img, cmap='tab10', vmin=0, vmax=8)\n",
    "    cbar_ax = fig.add_axes([1.02, 0.15, 0.015, 0.7])\n",
    "    fig.colorbar(cbar_img, cax=cbar_ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbedc4d-3871-4d34-9db1-cc765295a8f6",
   "metadata": {},
   "source": [
    "## First batch image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92baffb7-1b6c-43c2-bb4f-fd02313e084a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 4, figsize=(20, 10))\n",
    "ax = ax.flatten()\n",
    "for i in range(8):\n",
    "    image = first_batch_img[i]\n",
    "    show_image(image, ax[i])\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc85a535-a5d3-489e-bfb5-3a0ed2bb2ca1",
   "metadata": {},
   "source": [
    "## True mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69ac3f8-676f-4326-9c3b-9a0fd0bb79fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mask(first_batch_true_mask);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f337b101-6bec-48a2-8791-21ef64c738cb",
   "metadata": {},
   "source": [
    "## UNet predict mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80a6b5e-ca85-4431-8fde-94b37588c865",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mask(first_batch_pred_mask1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a25f096-843f-4e9d-b157-307548495724",
   "metadata": {},
   "source": [
    "## Multi kernel UNet predict mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534140ca-a0ce-4854-8613-ecc85458481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mask(first_batch_pred_mask2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f40761",
   "metadata": {},
   "source": [
    "## 感想\n",
    "マルチカーネル設計のほうが精度がいいが、それでも精度が低い。精度よくやっている人たちはどんな工夫をしているか、損失関数含めて気になった。<br>\n",
    "今度、ライブラリの内容を確認してみる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70913c4",
   "metadata": {},
   "source": [
    "\n",
    "## 🚘 自動運転でよく使われるセグメンテーションモデル\n",
    "\n",
    "### ✅ 1. **DeepLab v3+**\n",
    "\n",
    "* **特徴**：Atrous Convolution（空洞畳み込み） + Encoder-Decoder 構造\n",
    "* **利点**：高精度。遠方の物体や細かい構造にも強い\n",
    "* **用途**：自動運転の研究・プロトタイプによく使われる\n",
    "* **実装**：`torchvision.models.segmentation.deeplabv3_resnet101` など\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 2. **PSPNet (Pyramid Scene Parsing Network)**\n",
    "\n",
    "* **特徴**：ピラミッドプーリングモジュールで広範囲な文脈情報を取得\n",
    "* **利点**：シーン全体を把握したセグメンテーションに強い\n",
    "* **用途**：都市シーン理解（例：Cityscapes）に強い実績\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 3. **ENet**\n",
    "\n",
    "* **特徴**：軽量なリアルタイムセグメンテーションモデル（Speed重視）\n",
    "* **利点**：組み込みデバイス（NVIDIA Jetson等）に向く\n",
    "* **用途**：実走行車両でのテストや小型システムに最適\n",
    "* **フレームレート**：> 60 FPS on embedded devices\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 4. **BiSeNet (Bilateral Segmentation Network)**\n",
    "\n",
    "* **特徴**：空間情報と文脈情報を並列に扱う2ブランチ構成\n",
    "* **利点**：**リアルタイム性と高精度の両立**\n",
    "* **BiSeNetV2** はさらに高効率設計\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 5. **SegFormer（NVIDIA, 2021）**\n",
    "\n",
    "* **特徴**：**Transformerベース**。軽量 & 高精度\n",
    "* **利点**：バックボーンにCNNを使わずにTransformerで高性能\n",
    "* **用途**：SOTA性能 + 小型化（real-time対応）\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 モデル選択の目安\n",
    "\n",
    "| モデル         | 精度 | 推論速度 | 実用性 | 備考          |\n",
    "| ----------- | -- | ---- | --- | ----------- |\n",
    "| DeepLab v3+ | ◎  | △    | ◎   | 精度重視        |\n",
    "| PSPNet      | ◎  | △    | ◎   | 都市風景向き      |\n",
    "| ENet        | △  | ◎    | ◯   | 組み込み向け      |\n",
    "| BiSeNetV2   | ◎  | ◎    | ◎   | 実車搭載の選択肢にも  |\n",
    "| SegFormer   | ◎  | ◎    | ◎   | 最新。軽量 & 高性能 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🏙️ よく使われる自動運転データセット\n",
    "\n",
    "| データセット         | 特徴                |\n",
    "| -------------- | ----------------- |\n",
    "| **Cityscapes** | 都市部の高解像度画像（19クラス） |\n",
    "| **CamVid**     | 道路走行シーン（車載カメラ映像）  |\n",
    "| **KITTI**      | 車載センサーデータ（ラベル少なめ） |\n",
    "| **BDD100K**    | 多様な時間帯・天候で大規模     |\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 まとめ\n",
    "\n",
    "* **精度重視（研究用途）**：DeepLab v3+, PSPNet\n",
    "* **リアルタイム処理（実運用・組み込み）**：ENet, BiSeNetV2\n",
    "* **次世代モデルに注目**：SegFormer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
